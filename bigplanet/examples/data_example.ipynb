{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bigplanet Example\n",
    "\n",
    "David Fleming, July 2016\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, I'll run through the basic functionality of bigplanet, a package for data-processing, analysis, and plotting of data produced by VPLANET.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Imports\n",
    "%matplotlib inline\n",
    "\n",
    "from __future__ import print_function, division, absolute_import\n",
    "\n",
    "#Imports\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0) Import bigplanet\n",
    "\n",
    "---\n",
    "\n",
    "bigplanet, a dumb wordplay on ``big data`` and ``VPLANET``, is a suite of tools used to wrangle and analyze data produced by large-scale ``VPLANET`` simulation suites.  It is currently under active development and any/all suggestions, bug discoveries, pull requests, etc are very much appreciated!\n",
    "\n",
    "bigplanet can be imported and used just like any other python module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bigplanet import data_extraction as de\n",
    "from bigplanet import bigplot as bp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1) Load in Data\n",
    "\n",
    "---\n",
    "\n",
    "Let's say we ran a simulation where we varied several body parameters in some grid over parameters such as binary semimajor axis, eccentricity and the same for a circumbinary planet (CBP).  The suite of simulations, set up using ``vspace``, produced a lot of directories, one for each simulation, so in order to work with the data products, we need to traverse through each directory, extract the results, and transform them into a meaningful data structure.  \n",
    "\n",
    "Currently, ``bigplanet`` supports extracting data into a hdf5 dataset.  The hdf5 format is a versitile compressed file format that allows for quick access of data that cannot fit into memory.  The hdf5 format stores array-like data in a POSIX-like filetree system that supports random-access by decompressing the given data in real-time that is only a factor of a few slower than if one was accessing the data from memory.  Since VPLANET simulations have a simulation-body-variable hierarchy, it makes sense to use a hierarchical format.  \n",
    "\n",
    "For more info on all things hdf5, check out these links:\n",
    "\n",
    "hdf5 with python: http://docs.h5py.org/en/latest/quick.html\n",
    "\n",
    "hdf5 general: https://en.wikipedia.org/wiki/Hierarchical_Data_Format (the h, d, and f from hdf5!)\n",
    "\n",
    "hdf5 group: https://www.hdfgroup.org/HDF5/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Tell bigplanet where the data is **\n",
    "\n",
    "---\n",
    "\n",
    "Here, we'll load the data into the variable data.  When using the hdf5 format, data is actually an object that stores metadata about the hdf5 dataset for ease of manipulation.  For this, we must make known the source directory, src, the location of the hdf5 dataset, dataset, and the format we'll be using, fmt.  Other kwargs are remove_halts and skip_body.  Remove_halts = True makes the function ignore all simulations that halted while skip_body tells the function to ignore all the output from the specified bodies.  Cadence is an optional kwarg that has extract_data_hdf5 output which simulation it's on every cadence steps for integer cadence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define root dirctory where all sim sub directories are located\n",
    "src = \"/Users/dflemin3/Desktop/GM_run/\"\n",
    "\n",
    "# Path to the hdf5 dataset\n",
    "dataset= os.path.join(src,\"simulation.hdf5\")\n",
    "\n",
    "# How you wish the data to be ordered (grid for grid simulation suites)\n",
    "order = \"grid\"\n",
    "\n",
    "# Format of the data (default)\n",
    "fmt = \"hdf5\"\n",
    "\n",
    "# Ignore simulations that halted at some point?\n",
    "remove_halts = False\n",
    "\n",
    "# Any bodies whose output you wish to ignore?\n",
    "skip_body = [\"primary.in\"]\n",
    "\n",
    "# Any parameters to extract from input files?\n",
    "var_from_infile = {\"cbp\" : [\"iBodyType\"]}\n",
    "\n",
    "# An optional kwarg that has extract_data_hdf5 output which simulation it's on\n",
    "# every cadence steps for int cadence\n",
    "cadence = 100\n",
    "\n",
    "# Compression algorithm to use\n",
    "compression = \"gzip\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Load in the data!**\n",
    "\n",
    "---\n",
    "\n",
    "Here we actually get a data object to manipulate and work with.  This bit of code discriminates between the two formats and prints out useful information.  I should make this into a function.  The core of this \"function\" occurs in the following line\n",
    ">    data = de.extract_data_hdf5(src=src, dataset=dataset, ...)\n",
    "\n",
    "We pass the extract_data_hdf5 function the source directory src and the name of the dataset, dataset.  If the dataset does not exist, i.e. all the data directories in source have not been traversed and parsed, then the function will traverse the simulation directories and store the data into dataset.  On my slow laptop, this takes ~1.5 minutes for ~1000 simulations.  If dataset does exist, it's quickly loaded into data.  For suites with ~10,000 simulations, it takes roughly 30 minutes - hour depending on how bad your computer is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hdf5 dataset already exists.  Reading from: /Users/dflemin3/Desktop/GM_run/simulation\n",
      "Using metadata stored in dataset object.\n",
      "Name: /Users/dflemin3/Desktop/GM_run/simulation.hdf5. Size: 625. Order: none\n"
     ]
    }
   ],
   "source": [
    "data = de.extract_data_hdf5(src=src, dataset=dataset, order=order, remove_halts=remove_halts,\n",
    "                           skip_body=skip_body, compression=compression, var_from_infile=var_from_infile,\n",
    "                           cadence=cadence)\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Dataset object methods **\n",
    "\n",
    "---\n",
    "\n",
    "The dataset object has some useful methods and attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'cbp.in', u'primary.in', u'secondary.in']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.input_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{u'cbp.in': [u'Time',\n",
       "  u'LongP',\n",
       "  u'SemimajorAxis',\n",
       "  u'Eccentricity',\n",
       "  u'dIncBinary'],\n",
       " u'primary.in': [u'Time'],\n",
       " u'secondary.in': [u'Time',\n",
       "  u'Radius',\n",
       "  u'SemimajorAxis',\n",
       "  u'Eccentricity',\n",
       "  u'RotRate',\n",
       "  u'MeanMotion']}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.data_cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'GM_simbe0_ba1_cfe0_ca0'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Given a simulation number, what is the simulation's name?\n",
    "data.sim_name(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'none'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How is the data ordered?\n",
    "data.order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "625"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What is the size of the dataset (number of simulations)?\n",
    "data.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'/Users/dflemin3/Desktop/GM_run/simulation.hdf5'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# What's the path to the actual hdf5 file?\n",
    "data.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2) Data Processing\n",
    "\n",
    "---\n",
    "\n",
    "The real power of this package is the easy of data access and manipulation.  In this section, I'll show the user how to access the results of ``VPLANET`` simulations in a variety of ways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Individual simulation data**\n",
    "\n",
    "---\n",
    "\n",
    "Suppose you want to pull the binary (stored in the body \"secondary\") orbital eccentricity and the simulation time from the 2nd (index 1) simulation to plot how it varies as a function of time.  Accessing it is as easy as the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: [       0.  1000000.  2000000.  3000000.  4000000.  5000000.  6000000.\n",
      "  7000000.  8000000.  9000000.] ...\n",
      "Ecc: [ 0.01      0.008694  0.007955  0.007777  0.007695  0.007647  0.007616\n",
      "  0.007595  0.00758   0.007569] ...\n"
     ]
    }
   ],
   "source": [
    "time, ecc = data.get(0,\"secondary\",[\"Time\",\"Eccentricity\"])\n",
    "print(\"Time:\",time[0:10],\"...\")\n",
    "print(\"Ecc:\",ecc[0:10],\"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "de.data_from_dir_hdf5?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The get method for the data object takes the following arguments:\n",
    "```\n",
    "1) Simulation number : int\n",
    "2) body name : str\n",
    "3) parameter or list of parameters : str, list of str\n",
    "```\n",
    "\n",
    "This method allows for easy data access for individual simulations and allows for one to iterate over an arbitrary number of simulations.  This is useful if, for whatever reason, the user would want to plot binary eccentricity as a function of time from every simulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Documentation Notes\n",
    "\n",
    "---\n",
    "\n",
    "Note that by typing\n",
    "\n",
    ">some_bigplanet_func.?\n",
    "\n",
    "into an ipython cell, the docs will appear allowing the user to see what the function does, its input parameters, what they are, what the function returns, etc.  I've taken a lot of time to write out the docs so that the function's arguments and purpose are more clear.  Also, the user can also open up the source file to read further documentation that breaks down what each part of the function is doing.  Alternatively, you can always bug me (David) for an explanation, or to see if even I remember what I wrote."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Bulk Simulation Results**\n",
    "\n",
    "What if we don't want data from individual simulations, but rather some matrix-like structure that shows how one or many parameter changes as a function of initial simulation conditions.  For example, what if one wants to see how the CBP eccentricity at some given time varies as a function of binary a, e and CBP e. \n",
    "\n",
    "To do this, we'll use another function, aggregate_data, that uses the following calling sequence:\n",
    ">de.aggregate_data(data, bodies=bodies, ind=0, funcs={\"cbp\" : {\"Semim\" : np.mean}},\n",
    ">                    new_cols=new_cols,cache=\"cache.pkl\",fmt=fmt,**kw)\n",
    "\n",
    "\n",
    "Instead of actually producing a matrix, this function produces a dictionary of pandas dataframes where the key is the body name and the value is a pandas dataframe where the columns are the parameters varied over the simulation for the given body and any other user-defined columns (I'll discuss this later).\n",
    "\n",
    "This function can get as complex as the user wants it to get, so we'll break it down by input parameter:\n",
    "\n",
    "---\n",
    "\n",
    "1) data\n",
    "``` \n",
    "hdf5 dataset object\n",
    "``` \n",
    "2) bodies\n",
    "```\n",
    "dictionary where the keys are body names and and the values are lists of the initial parameters that were varied for said object.  In our typical example, binary eccentricity was varied in the vspace input file with some line like\n",
    "    secondary.in\n",
    "    dEcc [0.01,0.1,n5] dEcc\n",
    "    \n",
    "In the secondary.in file, eccentricity is outputted by specifying \"Eccentricity\" in the saOutputOrder line.\n",
    "Hence to have that parameter in the final matrix, we would need to include in the bodies dict\n",
    "\n",
    "\"secondary\" : ['Eccentricity']\n",
    "\n",
    "amongst other parameters of interest.  In the cell below our simulation grid spanned binary a, e and the same for the cbp and our bodies dict reflects that.\n",
    "```\n",
    "\n",
    "3) ind\n",
    "```\n",
    "Time index to extract for the given body's variable from each simulation, e.g. the binary's initial (time[ind = 0]) eccentricity.\n",
    "This makes it explicit that we want the matrix to span initial simulation conditions.  I'm not sure why/if a user would have their grid over other time, but this variable allows for that functionality.\n",
    "```\n",
    "\n",
    "4) funcs\n",
    "```\n",
    "Funcs is a nested dictionary of the form {body : {body_variable : function}, body2 : {...}, ...}.  If something in funcs is specified for the variable of a given body, the funcs functionality supercedes the ind behavior and stores the result of funcs of that variable time series instead of the initial condition.  \n",
    "\n",
    "In the example below, I use funcs to store the mean CBP semimajor axis instead of the initial condition.  Typically a user won't use this, but it could be useful.\n",
    "```\n",
    "\n",
    "5) new_cols\n",
    "```\n",
    "New_cols is another nest dictionary that is rather powerful and allows the user to apply any function to the simulation data to produce a new data product for each simulation.  The effect of this is to add another column to the final dataframe.\n",
    "\n",
    "In the example below, I want to add the initial eccentricity again for a trivial example.\n",
    "\n",
    "new_cols = {\"cbp\" : {\"InitEcc\" : trivial}}\n",
    "\n",
    "This tells aggregate_data that for the cbp body, I want a new column, \"InitEcc\", added and it's computed by the trivial function.  Obviously, this functionality becomes useful for non-trivial calculations like computing eccentricity damping times for the cbp.\n",
    "\n",
    "Note that any functions supplied to new_cols require the first 3 parameters to be\n",
    "\n",
    "data, sim, body\n",
    "\n",
    "and require a fmt kwarg. See the docs for more details.\n",
    "```\n",
    "\n",
    "6) kw\n",
    "```\n",
    "A dictionary of any keyword arguments the functions in new_cols require.\n",
    "```\n",
    "\n",
    "7) cache\n",
    "```\n",
    "Where to cache the output of this function into a pickle file.  This is useful if any of your new_cols functions are computationaly expensive and the pickle file can be transfered and read by all sorts of machines and python distros.\n",
    "```\n",
    "\n",
    "8) fmt\n",
    "```\n",
    "Tells the function whether you are using the hdf5 data format.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Example**\n",
    "\n",
    "---\n",
    "\n",
    "Ok, that's a lot of information, so let's see it in action.  To show off functionality, below I wrote a trivial function that returns the initial eccentricity and takes a keyword argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Trivial function to return initial eccentricity\n",
    "# This function assumes an hdf5 format\n",
    "def trivial(data,sim,body,key=None,fmt=\"hdf5\"):\n",
    "    # Check out the get function in action!\n",
    "    return data.get(sim,body,key)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define the bodies and body variables to extract using a dictionary\n",
    "bodies = {'cbp': ['Eccentricity', 'SemimajorAxis'],'secondary': ['SemimajorAxis','Eccentricity']}\n",
    "\n",
    "# Define the new value (dataframe column) to produce for a given body.  The new column\n",
    "# and how to calculate it are given as a dictionary for each body.\n",
    "\n",
    "# New column for the cbp is \"InitEcc\" and is calculated using the function \"trivial\"\n",
    "new_cols = {\"cbp\" : {\"InitEcc\" : trivial}}\n",
    "\n",
    "# Define any keyword arguments trivial might need\n",
    "kw = {\"key\" : \"Eccentricity\"}\n",
    "\n",
    "# Extract and save into a cache (or read from it if it already exists)\n",
    "# Note ind=0 makes it clear that we want initial conditions stored for all non-new_cols variables\n",
    "df = de.aggregate_data(data, bodies=bodies, ind=0, funcs={\"cbp\" : {\"SemimajorAxis\" : np.mean}},\n",
    "                    new_cols=new_cols,cache=src+\"trivial_cache.pkl\",fmt=fmt,**kw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** What does df look like?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Dataframe with new column produced by user-defined function!\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "** Oh no!  I forgot another column I wanted to add to the data frame.  **\n",
    "\n",
    "No problem, there's a function for that: add_column.\n",
    "\n",
    "For add_column, you need to pass it the dataframe you wish to add a column to, df, the path to a cache to re-cache the dataframe, and new_cols.  Note that new_cols is the exact same as the one used in aggregate_data.\n",
    "\n",
    "Let's define the function that computes that simulation metric we forgot to include."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mean_func(data,i,body,fmt=\"hdf5\",**kwargs):\n",
    "    return np.mean(data.get(i,body,\"Eccentricity\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# new_cols, define similar to the one in aggregate_data\n",
    "new_cols = {\"cbp\" : {\"MeanEcc\" : mean_func, \"MeanEcc_Again\" : mean_func}}\n",
    "\n",
    "# Optional kwargs\n",
    "kwargs = {}\n",
    "\n",
    "df = de.add_column(data, df, new_cols=new_cols, cache=src+\"trivial_cache.pkl\", **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Now what does the updated dataframe look like?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
